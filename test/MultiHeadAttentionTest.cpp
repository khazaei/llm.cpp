//
// Created by Hamidreza Khazaei on 4/21/24.
//

#include "Attention.h"
#include "Extension.h"
#include "catch2/catch_test_macros.hpp"

TEST_CASE("Test batch = 1, head = 1,  of multi head attention.") {
  constexpr auto seqLen = 6;
  constexpr auto embeddingDim = 3;
  const auto in =
      std::vector<float>{-1.1258398, -1.1523602, 0.5666506,  -1.1258398, -1.1523602,
                         0.5666506,  -1.1258398, -1.1523602, 0.5666506,

                         0.7935084,  0.5988395,  -1.5550951, 0.7935084,  0.5988395,
                         -1.5550951, 0.7935084,  0.5988395,  -1.5550951,

                         -0.3413604, 1.8530061,  0.4680964,  -0.3413604, 1.8530061,
                         0.4680964,  -0.3413604, 1.8530061,  0.4680964,

                         -0.1577124, -0.1733968, 0.1834779,  -0.1577124, -0.1733968,
                         0.1834779,  -0.1577124, -0.1733968, 0.1834779,

                         1.3893661,  1.5863342,  0.9462984,  1.3893661,  1.5863342,
                         0.9462984,  1.3893661,  1.5863342,  0.9462984,

                         -0.8436767, 0.9318266,  1.2590092,  -0.8436767, 0.9318266,
                         1.2590092,  -0.8436767, 0.9318266,  1.2590092};

  const auto out = std::vector<float>{
      -1.1258398, -1.1523602, 0.5666506, 0.7309045,  0.5417201, -1.4858896,
      -0.2562208, 1.5995227,  0.2628030, -0.3411601, 0.1019680, 0.0515932,
      0.9885026,  1.5208580,  0.7179147, -0.3141791, 1.0212752, 0.7984132};

  auto outAttention = std::vector<float>(static_cast<size_t>(embeddingDim) * seqLen);
  const auto outView = llm::view<float, 3>{outAttention.data(), 1, seqLen, embeddingDim};
  const auto inView = llm::view<const float, 3>{in.data(), 1, seqLen, 3 * embeddingDim};

  const auto numHeads = 1;
  llm::multiHeadAttentionCausal(outView, inView, numHeads);
  constexpr auto eps = 1e-6;
  CHECK(llm::isTensorsEqual(out, outAttention, eps));
}

TEST_CASE("Test batch = 1, head = 3, of multi head attention") {
  const auto in = std::vector<float>{
      0.4962566, 0.7682218, 0.0884774, 0.1320305, 0.3074228, 0.6340787, 0.4900934,
      0.8964447, 0.4556280, 0.6323063, 0.3488935, 0.4017173, 0.0223258, 0.1688589,
      0.2938884, 0.5185218, 0.6976676, 0.8000114, 0.1610295, 0.2822686, 0.6816086,
      0.9151940, 0.3970999, 0.8741559, 0.4194083, 0.5529070, 0.9527381, 0.0361648,
      0.1852310, 0.3734174, 0.3051000, 0.9320004, 0.1759102, 0.2698336, 0.1506798,
      0.0317195, 0.2081298, 0.9297990, 0.7231092, 0.7423363, 0.5262958, 0.2436582,
      0.5845923, 0.0331526, 0.1387169, 0.2422350, 0.8154690, 0.7931606, 0.2782525,
      0.4819588, 0.8197803, 0.9970666, 0.6984411, 0.5675464, 0.8352432, 0.2055988,
      0.5931720, 0.1123472, 0.1534569, 0.2417082, 0.7262365, 0.7010802, 0.2038237,
      0.6510535, 0.7744860, 0.4368913, 0.5190908, 0.6158524, 0.8101883, 0.9800971,
      0.1146882, 0.3167651, 0.6965050, 0.9142747, 0.9351037, 0.9411784, 0.5995073,
      0.0652087, 0.5459962, 0.1871973, 0.0340229, 0.9442462, 0.8801799, 0.0012360,
      0.5935860, 0.4157700, 0.4177194, 0.2711216, 0.6922781, 0.2038482, 0.6832957,
      0.7528540, 0.8579358, 0.6869556, 0.0051324, 0.1756516, 0.7496575, 0.6046507,
      0.1099580, 0.2120903, 0.9703746, 0.8369089, 0.2819874, 0.3741576, 0.0237010,
      0.4910129, 0.1234705, 0.1143216, 0.4724502, 0.5750725, 0.2952349, 0.7966888,
      0.1957304, 0.9536850, 0.8426499, 0.0783585, 0.3755578, 0.5225613, 0.5729505,
      0.6185871, 0.6962141, 0.5299501, 0.2560356, 0.7365945, 0.0203755, 0.2036467,
      0.3748351, 0.2564433, 0.3250833, 0.0901892, 0.3936424, 0.6068782, 0.1742671,
      0.4743403, 0.8579254, 0.4485999, 0.5138961, 0.4568655, 0.6011907, 0.8179197,
      0.9736231, 0.8175279, 0.9747068, 0.4638392, 0.0508392, 0.2629614, 0.8404526,
      0.4967588, 0.2514768, 0.1168441, 0.0320740, 0.0779959, 0.3985816, 0.7742030,
      0.7703205, 0.0177841, 0.8118910, 0.1087453, 0.3942949, 0.2972637, 0.4036924,
      0.4018286};

  const auto out = std::vector<float>{
      0.1610295, 0.2822686, 0.6816086, 0.9151940, 0.3970999, 0.8741559, 0.4194083,
      0.5529070, 0.9527381, 0.2041172, 0.5651852, 0.7407982, 0.5471206, 0.4461379,
      0.8427336, 0.6996289, 0.6235052, 0.7658825, 0.3756499, 0.6754045, 0.8065838,
      0.7126206, 0.4977171, 0.5667509, 0.6224525, 0.4672919, 0.5303556, 0.3405205,
      0.7688560, 0.8197187, 0.6017370, 0.4748967, 0.4218882, 0.5884267, 0.3828350,
      0.4334926, 0.3403816, 0.6683302, 0.7224301, 0.5051575, 0.4620013, 0.4434961,
      0.5172774, 0.3966154, 0.4936182, 0.4002910, 0.6833256, 0.6219555, 0.5567545,
      0.4183558, 0.4400183, 0.4843370, 0.3930716, 0.4928433};

  constexpr auto seqLen = size_t{6};
  constexpr auto embeddingDim = size_t{9};
  auto outAttention = std::vector<float>(embeddingDim * seqLen);
  const auto outView = llm::view<float, 3>{outAttention.data(), 1, seqLen, embeddingDim};
  const auto inView = llm::view<const float, 3>{in.data(), 1, seqLen, 3 * embeddingDim};

  constexpr auto numHeads = 3;
  llm::multiHeadAttentionCausal(outView, inView, numHeads);
  constexpr auto eps = 1e-6;
  CHECK(llm::isTensorsEqual(out, outAttention, eps));
}